{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import re\n",
    "import socket\n",
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "import regex\n",
    "import whois\n",
    "from bs4 import BeautifulSoup\n",
    "from tldextract import extract\n",
    "\n",
    "\n",
    "def generate_data_set(url):\n",
    "    data_set = []\n",
    "\n",
    "    # Converts the given URL into standard format\n",
    "    if not re.match(r\"^https?\", url):\n",
    "        url = \"http://\" + url\n",
    "\n",
    "    # Stores the response of the given URL\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except:\n",
    "\n",
    "        response = \"\"\n",
    "\n",
    "    # Extracts domain from the given URL\n",
    "    domain = re.findall(r\"://([^/]+)/?\", url)[0]\n",
    "\n",
    "    # Requests all the information about the domain\n",
    "    whois_response = requests.get(\"https://www.whois.com/whois/\" + domain)\n",
    "\n",
    "    rank_checker_response = requests.post(\"https://www.checkpagerank.net/index.php\", {\n",
    "        \"name\": domain\n",
    "    })\n",
    "\n",
    "    # Extracts global rank of the website\n",
    "    try:\n",
    "        global_rank = int(re.findall(r\"Global Rank: ([0-9]+)\", rank_checker_response.text)[0])\n",
    "    except:\n",
    "        global_rank = -1\n",
    "\n",
    "    def url_having_ip(url):\n",
    "        # using regular function\n",
    "        symbol = regex.findall(r'(http((s)?)://)((((\\d)+).)*)((\\w)+)(/((\\w)+))?', url)\n",
    "        if (len(symbol) != 0):\n",
    "            having_ip = 1  # phishing\n",
    "        else:\n",
    "            having_ip = -1  # legitimate\n",
    "            return (having_ip)\n",
    "        return 0\n",
    "\n",
    "    def url_length(url):\n",
    "        length = len(url)\n",
    "        if (length < 54):\n",
    "            return -1\n",
    "        elif (54 <= length <= 75):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def url_short(url):\n",
    "        if re.findall(\"goo.gl|bit.ly\", url):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def having_at_symbol(url):\n",
    "        symbol = regex.findall(r'@', url)\n",
    "        if (len(symbol) == 0):\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def doubleSlash(url):\n",
    "        if re.findall(r\"[^https?:]//\", url):\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def prefix_suffix(url):\n",
    "        subDomain, domain, suffix = extract(url)\n",
    "        if (domain.count('-')):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def sub_domain(url):\n",
    "        subDomain, domain, suffix = extract(url)\n",
    "        if (subDomain.count('.') == 0):\n",
    "            return -1\n",
    "        elif (subDomain.count('.') == 1):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def SSLfinal_State(url):\n",
    "        try:\n",
    "            # check wheather contains https\n",
    "            if (regex.search('^https', url)):\n",
    "                usehttps = 1\n",
    "            else:\n",
    "                usehttps = 0\n",
    "            # getting the certificate issuer to later compare with trusted issuer\n",
    "            # getting host name\n",
    "            subDomain, domain, suffix = extract(url)\n",
    "            host_name = domain + \".\" + suffix\n",
    "            context = ssl.create_default_context()\n",
    "            sct = context.wrap_socket(socket.socket(), server_hostname=host_name)\n",
    "            sct.connect((host_name, 443))\n",
    "            certificate = sct.getpeercert()\n",
    "            issuer = dict(x[0] for x in certificate['issuer'])\n",
    "            certificate_Auth = str(issuer['commonName'])\n",
    "            certificate_Auth = certificate_Auth.split()\n",
    "            if (certificate_Auth[0] == \"Network\" or certificate_Auth == \"Deutsche\"):\n",
    "                certificate_Auth = certificate_Auth[0] + \" \" + certificate_Auth[1]\n",
    "            else:\n",
    "                certificate_Auth = certificate_Auth[0]\n",
    "            trusted_Auth = ['Comodo', 'Symantec', 'GoDaddy', 'GlobalSign', 'DigiCert', 'StartCom', 'Entrust', 'Verizon',\n",
    "                            'Trustwave', 'Unizeto', 'Buypass', 'QuoVadis', 'Deutsche Telekom', 'Network Solutions',\n",
    "                            'SwissSign', 'IdenTrust', 'Secom', 'TWCA', 'GeoTrust', 'Thawte', 'Doster', 'VeriSign']\n",
    "            # getting age of certificate\n",
    "            startingDate = str(certificate['notBefore'])\n",
    "            endingDate = str(certificate['notAfter'])\n",
    "            startingYear = int(startingDate.split()[3])\n",
    "            endingYear = int(endingDate.split()[3])\n",
    "            Age_of_certificate = endingYear - startingYear\n",
    "\n",
    "            # checking final conditions\n",
    "            if ((usehttps == 1) and (certificate_Auth in trusted_Auth) and (Age_of_certificate >= 1)):\n",
    "                return -1  # legitimate\n",
    "            elif ((usehttps == 1) and (certificate_Auth not in trusted_Auth)):\n",
    "                return 0  # suspicious\n",
    "            else:\n",
    "                return 1  # phishing\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            return 1\n",
    "\n",
    "    def domain_registration(url):\n",
    "        try:\n",
    "            w = whois.whois(url)\n",
    "            updated = w.updated_date\n",
    "            exp = w.expiration_date\n",
    "            length = (exp[0] - updated[0]).days\n",
    "            if (length <= 365):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def favicon(url):\n",
    "        # ongoing\n",
    "        return 0\n",
    "\n",
    "    def port(url):\n",
    "        try:\n",
    "            port = domain.split(\":\")[1]\n",
    "            if port:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        except:\n",
    "            return -1\n",
    "\n",
    "    def https_token(url):\n",
    "        subDomain, domain, suffix = extract(url)\n",
    "        host = subDomain + '.' + domain + '.' + suffix\n",
    "        if (host.count('https')):  # attacker can trick by putting https in domain part\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def request_url(url):\n",
    "        try:\n",
    "            subDomain, domain, suffix = extract(url)\n",
    "            websiteDomain = domain\n",
    "\n",
    "            opener = urllib.request.urlopen(url).read()\n",
    "            soup = BeautifulSoup(opener, 'lxml')\n",
    "            imgs = soup.findAll('img', src=True)\n",
    "            total = len(imgs)\n",
    "\n",
    "            linked_to_same = 0\n",
    "            avg = 0\n",
    "            for image in imgs:\n",
    "                subDomain, domain, suffix = extract(image['src'])\n",
    "                imageDomain = domain\n",
    "                if (websiteDomain == imageDomain or imageDomain == ''):\n",
    "                    linked_to_same = linked_to_same + 1\n",
    "            vids = soup.findAll('video', src=True)\n",
    "            total = total + len(vids)\n",
    "\n",
    "            for video in vids:\n",
    "                subDomain, domain, suffix = extract(video['src'])\n",
    "                vidDomain = domain\n",
    "                if (websiteDomain == vidDomain or vidDomain == ''):\n",
    "                    linked_to_same = linked_to_same + 1\n",
    "            linked_outside = total - linked_to_same\n",
    "            if (total != 0):\n",
    "                avg = linked_outside / total\n",
    "\n",
    "            if (avg < 0.22):\n",
    "                return -1\n",
    "            elif (0.22 <= avg <= 0.61):\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def url_of_anchor(url):\n",
    "        try:\n",
    "            subDomain, domain, suffix = extract(url)\n",
    "            websiteDomain = domain\n",
    "\n",
    "            opener = urllib.request.urlopen(url).read()\n",
    "            soup = BeautifulSoup(opener, 'lxml')\n",
    "            anchors = soup.findAll('a', href=True)\n",
    "            total = len(anchors)\n",
    "            linked_to_same = 0\n",
    "            avg = 0\n",
    "            for anchor in anchors:\n",
    "                subDomain, domain, suffix = extract(anchor['href'])\n",
    "                anchorDomain = domain\n",
    "                if (websiteDomain == anchorDomain or anchorDomain == ''):\n",
    "                    linked_to_same = linked_to_same + 1\n",
    "            linked_outside = total - linked_to_same\n",
    "            if (total != 0):\n",
    "                avg = linked_outside / total\n",
    "\n",
    "            if (avg < 0.31):\n",
    "                return -1\n",
    "            elif (0.31 <= avg <= 0.67):\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def Links_in_tags(url):\n",
    "        try:\n",
    "            opener = urllib.request.urlopen(url).read()\n",
    "            soup = BeautifulSoup(opener, 'lxml')\n",
    "\n",
    "            no_of_meta = 0\n",
    "            no_of_link = 0\n",
    "            no_of_script = 0\n",
    "            anchors = 0\n",
    "            avg = 0\n",
    "            for meta in soup.find_all('meta'):\n",
    "                no_of_meta = no_of_meta + 1\n",
    "            for link in soup.find_all('link'):\n",
    "                no_of_link = no_of_link + 1\n",
    "            for script in soup.find_all('script'):\n",
    "                no_of_script = no_of_script + 1\n",
    "            for anchor in soup.find_all('a'):\n",
    "                anchors = anchors + 1\n",
    "            total = no_of_meta + no_of_link + no_of_script + anchors\n",
    "            tags = no_of_meta + no_of_link + no_of_script\n",
    "            if (total != 0):\n",
    "                avg = tags / total\n",
    "\n",
    "            if (avg < 0.25):\n",
    "                return -1\n",
    "            elif (0.25 <= avg <= 0.81):\n",
    "                return 0\n",
    "            else:\n",
    "                return 1\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def sfh(url):\n",
    "        # ongoing\n",
    "        return 0\n",
    "\n",
    "    def email_submit(url):\n",
    "        try:\n",
    "            opener = urllib.request.urlopen(url).read()\n",
    "            soup = BeautifulSoup(opener, 'lxml')\n",
    "            if (soup.find('mailto:')):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    def abnormal_url(url):\n",
    "        return 0;\n",
    "\n",
    "    def redirect(url):\n",
    "        if len(response.history) <= 1:\n",
    "            return -1\n",
    "        elif len(response.history) <= 4:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def on_mouseover(url):\n",
    "        if re.findall(\"<script>.+onmouseover.+</script>\", response.text):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def popup(url):\n",
    "        if re.findall(r\"alert\\(\", response.text):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def iframe(url):\n",
    "        if re.findall(r\"[<iframe>|<frameBorder>]\", response.text):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def age_of_domain(url):\n",
    "        try:\n",
    "            w = whois.whois(url)\n",
    "            start_date = w.creation_date\n",
    "            current_date = datetime.datetime.now()\n",
    "            age = (current_date - start_date[0]).days\n",
    "            if (age >= 180):\n",
    "                return -1\n",
    "            else:\n",
    "                return 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return 0\n",
    "\n",
    "    def dns(url):\n",
    "        # ongoing\n",
    "        return 0\n",
    "\n",
    "    def web_traffic(url):\n",
    "        try:\n",
    "            if global_rank > 0 and global_rank < 100000:\n",
    "                return -1\n",
    "            else:\n",
    "                return 1\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "    def page_rank(url):\n",
    "        try:\n",
    "            if global_rank > 0 and global_rank < 100000:\n",
    "                return -1\n",
    "            else:\n",
    "                return 1\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "    def google_index(url):\n",
    "        try:\n",
    "            if global_rank > 0 and global_rank < 100000:\n",
    "                return -1\n",
    "            else:\n",
    "                return 1\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "    def links_pointing(url):\n",
    "        number_of_links = len(re.findall(r\"<a href=\", response.text))\n",
    "        if number_of_links == 0:\n",
    "            return 1\n",
    "        elif number_of_links <= 2:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def rightClick(url):\n",
    "        if re.findall(r\"event.button ?== ?2\", response.text):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def statistical(url):\n",
    "        # ongoing\n",
    "        return -1\n",
    "\n",
    "    check = [[url_having_ip(url), url_length(url), url_short(url), having_at_symbol(url),\n",
    "              doubleSlash(url), prefix_suffix(url), sub_domain(url), SSLfinal_State(url),\n",
    "              domain_registration(url), favicon(url), port(url), https_token(url), request_url(url),\n",
    "              url_of_anchor(url), Links_in_tags(url), sfh(url), email_submit(url), abnormal_url(url),\n",
    "              redirect(url), on_mouseover(url), rightClick(url), popup(url), iframe(url),\n",
    "              age_of_domain(url), dns(url), web_traffic(url), page_rank(url), google_index(url),\n",
    "              links_pointing(url), statistical(url)]]\n",
    "    return check\n",
    "def main(url):\n",
    "    return generate_data_set(url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
